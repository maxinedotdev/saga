# Base directory for all data storage (documents, uploads, vector DB, etc.)
# Use ~/.doc-mcp/ for shared access between clients
# MCP_BASE_DIR=~/.doc-mcp/
MCP_BASE_DIR=~/.saga/

# LanceDB vector database configuration
# MCP_LANCE_DB_PATH is optional. If not set, defaults to $MCP_BASE_DIR/data/lancedb
# Only set this if you need to use a custom location
# MCP_LANCE_DB_PATH=~/.saga/data/lancedb

# Embedding provider configuration
# OpenAI-compatible provider only (e.g., LM Studio on http://127.0.0.1:1234)
# Default model is text-embedding-multilingual-e5-large-instruct (multilingual, high performance)
# Override with MCP_EMBEDDING_MODEL if you prefer a different model
MCP_EMBEDDING_PROVIDER=openai
MCP_EMBEDDING_MODEL=text-embedding-multilingual-e5-large-instruct
MCP_EMBEDDING_BASE_URL=http://127.0.0.1:1234
MCP_EMBEDDING_API_KEY=your-embedding-api-key
MCP_INDEXING_ENABLED=true
MCP_CACHE_SIZE=1000
MCP_PARALLEL_ENABLED=true
MCP_MAX_WORKERS=4
MCP_STREAMING_ENABLED=true
MCP_STREAM_CHUNK_SIZE=65536
MCP_STREAM_FILE_SIZE_LIMIT=10485760

# Chunking Configuration
# MCP_CHUNK_SIZE and MCP_CHUNK_OVERLAP can override intelligent defaults
# Set to -1 to use intelligent chunker's content-type-specific defaults
MCP_CHUNK_SIZE=-1
MCP_CHUNK_OVERLAP=-1

# Similarity threshold for search results (0.0 to 1.0, where 1.0 = perfect match)
# Default: 0.0 (allow all results)
MCP_SIMILARITY_THRESHOLD=0.0

# Default maximum search results returned
MCP_MAX_SEARCH_RESULTS=10

# Number of parallel workers for chunking (adjust based on system resources)
# Recommended: 2-4 for 4-8GB RAM, 4-8 for 16+GB RAM
MCP_MAX_WORKERS=4

# Batch size for embedding generation (reduces API calls)
MCP_EMBEDDING_BATCH_SIZE=10

# AI provider configuration (OpenAI-compatible only)
# Supports LM Studio (local) and synthetic.new (remote)
MCP_AI_BASE_URL=http://127.0.0.1:1234
MCP_AI_MODEL=ministral-3-8b-instruct-2512
MCP_AI_API_KEY=your-openai-compatible-api-key
MCP_AI_MAX_CONTEXT_CHUNKS=12

# Tag generation configuration
# Enable automatic tag generation using AI provider (default: false)
# When enabled, tags are generated asynchronously in the background when documents are added
MCP_TAG_GENERATION_ENABLED=false

# Use AI-generated tags in query search filters (default: false)
# When enabled, query filters will match documents with both manual tags and AI-generated tags
MCP_GENERATED_TAGS_IN_QUERY=false

# ============================================================================
# Multi-Provider Configuration (NEW)
# ============================================================================
# Configure multiple embedding providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_EMBEDDING_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_EMBEDDING_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "text-embedding-multilingual-e5-large-instruct"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.openai.com/v1",
#     "model": "text-embedding-3-small",
#     "apiKey": "sk-..."
#   }
# ]'

# Configure multiple AI search providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_AI_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_AI_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "ministral-3-8b-instruct-2512"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.synthetic.new/openai/v1",
#     "model": "glm-4.7",
#     "apiKey": "sk-..."
#   }
# ]'

# Provider health check configuration
# Number of consecutive failures before marking a provider as unhealthy (default: 3)
MCP_PROVIDER_FAILURE_THRESHOLD=3

# Time in milliseconds before retrying an unhealthy provider (default: 300000 = 5 minutes)
MCP_PROVIDER_RECOVERY_TIMEOUT=300000

# ============================================================================
# Request Timeout Configuration
# ============================================================================
# All timeout values are in milliseconds (positive integers only)
# Timeout hierarchy: specific → global → default (30000ms)

# Global default timeout for all HTTP requests (default: 30000 = 30 seconds)
# This value is used when no specific timeout is configured
# MCP_REQUEST_TIMEOUT_MS=30000

# AI search specific timeout (optional)
# Overrides global timeout for AI search requests (search_documents_with_ai)
# Useful when AI providers are slow or have variable response times
# MCP_AI_SEARCH_TIMEOUT_MS=60000

# Embedding generation specific timeout (optional)
# Overrides global timeout for embedding API requests
# Increase this if using large models or slow embedding providers
# MCP_EMBEDDING_TIMEOUT_MS=45000

# Example configurations:
# - Fast local setup: MCP_REQUEST_TIMEOUT_MS=15000
# - Slow remote APIs: MCP_REQUEST_TIMEOUT_MS=60000
# - Different timeouts per operation:
#   MCP_REQUEST_TIMEOUT_MS=30000
#   MCP_AI_SEARCH_TIMEOUT_MS=120000
#   MCP_EMBEDDING_TIMEOUT_MS=45000
# Note: Values must be positive integers. Non-numeric, zero, or negative values
# will be rejected and the fallback will be used instead.

# ============================================================================
# Reranking Configuration
# ============================================================================
# Enable/disable reranking (default: true - opt-out)
# When enabled, queries use two-stage retrieval: vector search → reranking
# Set to 'false' to disable and use vector-only search
MCP_RERANKING_ENABLED=true

# Reranking provider: 'cohere', 'jina', 'openai', 'custom', or 'lmstudio' (default: 'cohere')
# IMPORTANT: The provider must match the base URL for reranking to work correctly
MCP_RERANKING_PROVIDER=cohere

# API base URL for reranking provider
# IMPORTANT: This URL must match your chosen provider!
# - Cohere: https://api.cohere.ai/v1 (requires API key)
# - Jina AI: https://api.jina.ai/v1 (requires API key)
# - OpenAI: https://api.openai.com/v1 (requires API key)
# - LM Studio (local): http://localhost:1234/v1 or http://127.0.0.1:1234/v1 (no API key needed)
# - Custom: Your custom OpenAI-compatible endpoint
MCP_RERANKING_BASE_URL=https://api.cohere.ai/v1

# API key for reranking provider
# REQUIRED for: Cohere, Jina AI, OpenAI
# OPTIONAL for: Custom endpoints (check your provider's requirements)
# NOT NEEDED for: LM Studio (local)
# 
# IMPORTANT: Replace 'your-reranking-api-key' with your actual API key
# For Cohere: Get your key at https://dashboard.cohere.ai/api-keys
# For Jina AI: Get your key at https://jina.ai/
# For OpenAI: Get your key at https://platform.openai.com/api-keys
MCP_RERANKING_API_KEY=your-reranking-api-key

# Reranking model to use
# IMPORTANT: The model must be supported by your chosen provider
# - Cohere: rerank-multilingual-v3.0 (recommended for multilingual support)
#           rerank-english-v3.0 (English-only, faster)
# - Jina AI: jina-reranker-v3-mlx
# - OpenAI: gpt-4o, gpt-4o-mini, or any model that supports reranking
# - LM Studio: Any instruction-following model (e.g., ministral-3-8b-instruct-2512)
#              Uses chat completions endpoint for reranking
# - Custom: Your custom model name
MCP_RERANKING_MODEL=rerank-multilingual-v3.0

# Maximum number of candidates to rerank (default: 50)
# Higher values = better quality but slower
# Recommended: 50 for top 10 results, 100 for top 20 results
MCP_RERANKING_CANDIDATES=50

# Number of results to return after reranking (default: 10)
# Must be less than or equal to MCP_RERANKING_CANDIDATES
MCP_RERANKING_TOP_K=10

# Request timeout for reranking API calls in milliseconds (default: 30000)
# Increase if using slow providers or large candidate counts
MCP_RERANKING_TIMEOUT=30000

# ============================================================================
# Example Configurations (choose one and uncomment)
# ============================================================================

# Example 1: Cohere (recommended for multilingual support)
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=cohere
# MCP_RERANKING_BASE_URL=https://api.cohere.ai/v1
# MCP_RERANKING_API_KEY=your-cohere-api-key-here
# MCP_RERANKING_MODEL=rerank-multilingual-v3.0

# Example 2: Jina AI (optimized for code and technical docs)
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=jina
# MCP_RERANKING_BASE_URL=https://api.jina.ai/v1
# MCP_RERANKING_API_KEY=your-jina-api-key-here
# MCP_RERANKING_MODEL=jina-reranker-v3-mlx

# Example 3: OpenAI
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=openai
# MCP_RERANKING_BASE_URL=https://api.openai.com/v1
# MCP_RERANKING_API_KEY=sk-your-openai-api-key-here
# MCP_RERANKING_MODEL=gpt-4o

# Example 4: LM Studio (local, uses chat completions for reranking)
# IMPORTANT: LM Studio doesn't support /v1/rerank endpoint
# Instead, it uses /v1/chat/completions with a custom prompt to rank documents
# 
# Prerequisites:
# - LM Studio server must be running on the specified port
# - Load an instruction-following model (e.g., ministral-3-8b-instruct-2512)
# - The model should be capable of understanding ranking instructions
#
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=lmstudio
# MCP_RERANKING_BASE_URL=http://localhost:1234/v1
# MCP_RERANKING_API_KEY=
# MCP_RERANKING_MODEL=ministral-3-8b-instruct-2512
#
# Note: LM Studio reranking uses chat completions, which may be slower than
# dedicated reranking APIs. Consider increasing MCP_RERANKING_TIMEOUT if needed.

# Example 5: Custom OpenAI-compatible endpoint
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=custom
# MCP_RERANKING_BASE_URL=https://your-custom-endpoint.com/v1
# MCP_RERANKING_API_KEY=your-custom-api-key-if-needed
# MCP_RERANKING_MODEL=your-model-name

# ============================================================================
# Troubleshooting Tips
# ============================================================================
# If reranking is not working:
# 1. Check that MCP_RERANKING_ENABLED=true
# 2. Verify MCP_RERANKING_PROVIDER matches your base URL
# 3. Ensure MCP_RERANKING_API_KEY is set (unless using LM Studio or custom)
# 4. Confirm MCP_RERANKING_MODEL is supported by your provider
# 5. Check the server logs for detailed error messages
# 6. Test your API key and endpoint with curl or a similar tool
# 7. If using LM Studio:
#    - Ensure the server is running on the specified port
#    - Verify an instruction-following model is loaded
#    - Consider increasing MCP_RERANKING_TIMEOUT (chat completions may be slower)
#    - Check that the model can understand and follow ranking instructions
