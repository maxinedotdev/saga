# Base directory for all data storage (documents, uploads, vector DB, etc.)
# Use ~/.doc-mcp/ for shared access between clients
# MCP_BASE_DIR=~/.doc-mcp/
MCP_BASE_DIR=~/.doc-mcp/

# Vector database configuration
MCP_VECTOR_DB=lance
# MCP_LANCE_DB_PATH is optional. If not set, defaults to $MCP_BASE_DIR/data/lancedb
# Only set this if you need to use a custom location
# MCP_LANCE_DB_PATH=~/.doc-mcp/data/lancedb

# Embedding provider configuration
MCP_EMBEDDING_PROVIDER=transformers
MCP_EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2
MCP_EMBEDDING_BASE_URL=http://127.0.0.1:1234
MCP_EMBEDDING_API_KEY=your-embedding-api-key
MCP_INDEXING_ENABLED=true
MCP_CACHE_SIZE=1000
MCP_PARALLEL_ENABLED=true
MCP_MAX_WORKERS=4
MCP_STREAMING_ENABLED=true
MCP_STREAM_CHUNK_SIZE=65536
MCP_STREAM_FILE_SIZE_LIMIT=10485760

# Chunking Configuration
# MCP_CHUNK_SIZE and MCP_CHUNK_OVERLAP can override intelligent defaults
# Set to -1 to use intelligent chunker's content-type-specific defaults
MCP_CHUNK_SIZE=-1
MCP_CHUNK_OVERLAP=-1

# Minimum similarity score for search results (0.0 to 1.0)
# Lower = more results, Higher = more precise
MCP_SIMILARITY_THRESHOLD=0.5

# Default maximum search results returned
MCP_MAX_SEARCH_RESULTS=10

# Number of parallel workers for chunking (adjust based on system resources)
# Recommended: 2-4 for 4-8GB RAM, 4-8 for 16+GB RAM
MCP_MAX_WORKERS=4

# Batch size for embedding generation (reduces API calls)
MCP_EMBEDDING_BATCH_SIZE=10

# AI provider configuration
MCP_AI_PROVIDER=gemini
GEMINI_API_KEY=your-gemini-api-key

# OpenAI-compatible provider (LM Studio / synthetic.new)
MCP_AI_BASE_URL=http://127.0.0.1:1234
MCP_AI_MODEL=ministral-3-8b-instruct-2512
MCP_AI_API_KEY=your-openai-compatible-api-key
MCP_AI_MAX_CONTEXT_CHUNKS=12
