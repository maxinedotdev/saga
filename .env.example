# Base directory for all data storage (documents, uploads, vector DB, etc.)
# Use ~/.doc-mcp/ for shared access between clients
# MCP_BASE_DIR=~/.doc-mcp/
MCP_BASE_DIR=~/.saga/

# LanceDB vector database configuration
# MCP_LANCE_DB_PATH is optional. If not set, defaults to $MCP_BASE_DIR/data/lancedb
# Only set this if you need to use a custom location
# MCP_LANCE_DB_PATH=~/.saga/data/lancedb

# Embedding provider configuration
# OpenAI-compatible provider only (e.g., LM Studio on http://127.0.0.1:1234)
# Default model is text-embedding-multilingual-e5-large-instruct (multilingual, high performance)
# Override with MCP_EMBEDDING_MODEL if you prefer a different model
MCP_EMBEDDING_PROVIDER=openai
MCP_EMBEDDING_MODEL=text-embedding-multilingual-e5-large-instruct
MCP_EMBEDDING_BASE_URL=http://127.0.0.1:1234
MCP_EMBEDDING_API_KEY=your-embedding-api-key
MCP_INDEXING_ENABLED=true
MCP_CACHE_SIZE=1000
MCP_PARALLEL_ENABLED=true
MCP_MAX_WORKERS=4
MCP_STREAMING_ENABLED=true
MCP_STREAM_CHUNK_SIZE=65536
MCP_STREAM_FILE_SIZE_LIMIT=10485760

# Chunking Configuration
# MCP_CHUNK_SIZE and MCP_CHUNK_OVERLAP can override intelligent defaults
# Set to -1 to use intelligent chunker's content-type-specific defaults
MCP_CHUNK_SIZE=-1
MCP_CHUNK_OVERLAP=-1

# Similarity threshold for search results (0.0 to 1.0, where 1.0 = perfect match)
# Default: 0.0 (allow all results)
MCP_SIMILARITY_THRESHOLD=0.0

# Default maximum search results returned
MCP_MAX_SEARCH_RESULTS=10

# Number of parallel workers for chunking (adjust based on system resources)
# Recommended: 2-4 for 4-8GB RAM, 4-8 for 16+GB RAM
MCP_MAX_WORKERS=4

# Batch size for embedding generation (reduces API calls)
MCP_EMBEDDING_BATCH_SIZE=10

# AI provider configuration (OpenAI-compatible only)
# Supports LM Studio (local) and synthetic.new (remote)
MCP_AI_BASE_URL=http://127.0.0.1:1234
MCP_AI_MODEL=ministral-3-8b-instruct-2512
MCP_AI_API_KEY=your-openai-compatible-api-key
MCP_AI_MAX_CONTEXT_CHUNKS=12

# Tag generation configuration
# Enable automatic tag generation using AI provider (default: false)
# When enabled, tags are generated asynchronously in the background when documents are added
MCP_TAG_GENERATION_ENABLED=false

# Use AI-generated tags in query search filters (default: false)
# When enabled, query filters will match documents with both manual tags and AI-generated tags
MCP_GENERATED_TAGS_IN_QUERY=false

# ============================================================================
# Multi-Provider Configuration (NEW)
# ============================================================================
# Configure multiple embedding providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_EMBEDDING_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_EMBEDDING_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "text-embedding-multilingual-e5-large-instruct"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.openai.com/v1",
#     "model": "text-embedding-3-small",
#     "apiKey": "sk-..."
#   }
# ]'

# Configure multiple AI search providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_AI_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_AI_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "ministral-3-8b-instruct-2512"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.synthetic.new/openai/v1",
#     "model": "glm-4.7",
#     "apiKey": "sk-..."
#   }
# ]'

# Provider health check configuration
# Number of consecutive failures before marking a provider as unhealthy (default: 3)
MCP_PROVIDER_FAILURE_THRESHOLD=3

# Time in milliseconds before retrying an unhealthy provider (default: 300000 = 5 minutes)
MCP_PROVIDER_RECOVERY_TIMEOUT=300000

# ============================================================================
# Request Timeout Configuration
# ============================================================================
# All timeout values are in milliseconds (positive integers only)
# Timeout hierarchy: specific → global → default (30000ms)

# Global default timeout for all HTTP requests (default: 30000 = 30 seconds)
# This value is used when no specific timeout is configured
# MCP_REQUEST_TIMEOUT_MS=30000

# AI search specific timeout (optional)
# Overrides global timeout for AI search requests (search_documents_with_ai)
# Useful when AI providers are slow or have variable response times
# MCP_AI_SEARCH_TIMEOUT_MS=60000

# Embedding generation specific timeout (optional)
# Overrides global timeout for embedding API requests
# Increase this if using large models or slow embedding providers
# MCP_EMBEDDING_TIMEOUT_MS=45000

# Example configurations:
# - Fast local setup: MCP_REQUEST_TIMEOUT_MS=15000
# - Slow remote APIs: MCP_REQUEST_TIMEOUT_MS=60000
# - Different timeouts per operation:
#   MCP_REQUEST_TIMEOUT_MS=30000
#   MCP_AI_SEARCH_TIMEOUT_MS=120000
#   MCP_EMBEDDING_TIMEOUT_MS=45000
# Note: Values must be positive integers. Non-numeric, zero, or negative values
# will be rejected and the fallback will be used instead.

# ============================================================================
# Reranking Configuration
# ============================================================================
# Enable/disable reranking (default: true - opt-out)
# When enabled, queries use two-stage retrieval: vector search → reranking
# Set to 'false' to disable and use vector-only search
MCP_RERANKING_ENABLED=true

# Reranking provider: 'cohere', 'jina', 'openai', or 'custom' (default: 'cohere')
MCP_RERANKING_PROVIDER=cohere

# API base URL for reranking provider
# Default: http://localhost:1234/v1 (LM Studio)
# Cohere: https://api.cohere.ai/v1
# Jina AI: https://api.jina.ai/v1
# OpenAI: https://api.openai.com/v1
# Custom: your custom endpoint
MCP_RERANKING_BASE_URL=http://localhost:1234/v1

# API key for reranking provider
# Required for all providers except 'custom'
MCP_RERANKING_API_KEY=your-reranking-api-key

# Reranking model to use
# Cohere: rerank-multilingual-v3.0 (recommended for multilingual support)
# Jina AI: jina-reranker-v3-mlx
# OpenAI: custom model name
# Custom: your custom model name
MCP_RERANKING_MODEL=rerank-multilingual-v3.0

# Maximum number of candidates to rerank (default: 50)
# Higher values = better quality but slower
# Recommended: 50 for top 10 results, 100 for top 20 results
MCP_RERANKING_CANDIDATES=50

# Number of results to return after reranking (default: 10)
# Must be less than or equal to MCP_RERANKING_CANDIDATES
MCP_RERANKING_TOP_K=10

# Request timeout for reranking API calls in milliseconds (default: 30000)
# Increase if using slow providers or large candidate counts
MCP_RERANKING_TIMEOUT=30000

# Example configurations:
# - Cohere (recommended for multilingual):
#   MCP_RERANKING_PROVIDER=cohere
#   MCP_RERANKING_BASE_URL=https://api.cohere.ai/v1
#   MCP_RERANKING_API_KEY=your-cohere-api-key
#   MCP_RERANKING_MODEL=rerank-multilingual-v3.0
#
# - Jina AI (optimized for code and technical docs):
#   MCP_RERANKING_PROVIDER=jina
#   MCP_RERANKING_BASE_URL=https://api.jina.ai/v1
#   MCP_RERANKING_API_KEY=your-jina-api-key
#   MCP_RERANKING_MODEL=jina-reranker-v3-mlx
#
# - OpenAI-compatible:
#   MCP_RERANKING_PROVIDER=openai
#   MCP_RERANKING_BASE_URL=https://api.openai.com/v1
#   MCP_RERANKING_API_KEY=sk-...
#   MCP_RERANKING_MODEL=gpt-4o
#
# - Custom endpoint:
#   MCP_RERANKING_PROVIDER=custom
#   MCP_RERANKING_BASE_URL=https://your-custom-endpoint.com/v1
#   MCP_RERANKING_MODEL=your-model-name
#   MCP_RERANKING_API_KEY=optional-api-key
