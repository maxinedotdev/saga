# Base directory for all data storage (documents, uploads, vector DB, etc.)
# Use a shared base dir between clients (default: ~/.saga/)
# MCP_BASE_DIR=~/.saga/
MCP_BASE_DIR=~/.saga/

# LanceDB vector database configuration
# MCP_LANCE_DB_PATH is optional. If not set, defaults to $MCP_BASE_DIR/lancedb
# Only set this if you need to use a custom location
# MCP_LANCE_DB_PATH=~/.saga/lancedb

# Embedding provider configuration
# OpenAI-compatible provider only (e.g., LM Studio on http://localhost:1234)
# Default model is llama-nemotron-embed-1b-v2 (fast, high quality, 2048 dims)
# Override with MCP_EMBEDDING_MODEL if you prefer a different model
MCP_EMBEDDING_PROVIDER=openai
MCP_EMBEDDING_MODEL=llama-nemotron-embed-1b-v2
MCP_EMBEDDING_BASE_URL=http://localhost:1234
MCP_EMBEDDING_API_KEY=your-embedding-api-key

# Optional: vLLM-metal auto-configuration (Apple Silicon)
# When enabled, Saga can auto-configure a local vLLM OpenAI-compatible endpoint.
# MCP_EMBEDDING_VLLM_AUTO_START defaults to true unless explicitly set to false.
MCP_EMBEDDING_AUTO_CONFIGURE_VLLM=true
MCP_EMBEDDING_VLLM_AUTO_START=true
# Override base URL / port if needed
MCP_EMBEDDING_VLLM_BASE_URL=http://127.0.0.1:8000
MCP_EMBEDDING_VLLM_PORT=8000
# Path to the HF model directory used by vLLM-metal
MCP_EMBEDDING_VLLM_MODEL_PATH=~/.saga/models/llama-nemotron-embed-1b-v2
MCP_EMBEDDING_VLLM_MODEL=llama-nemotron-embed-1b-v2
# Embedding cache toggle (default: true)
MCP_CACHE_ENABLED=true
MCP_INDEXING_ENABLED=true
MCP_CACHE_SIZE=1000
MCP_PARALLEL_ENABLED=true
MCP_STREAMING_ENABLED=true
MCP_STREAM_CHUNK_SIZE=65536
MCP_STREAM_FILE_SIZE_LIMIT=104857600

# Chunking Configuration
# MCP_CHUNK_SIZE and MCP_CHUNK_OVERLAP can override intelligent defaults
# Set to -1 to use intelligent chunker's content-type-specific defaults
MCP_CHUNK_SIZE=-1
MCP_CHUNK_OVERLAP=-1

# Language Detection Configuration
# Accepted languages for ingestion (comma-separated ISO 639-1 codes)
# MCP_ACCEPTED_LANGUAGES=en,no
# Default query languages (comma-separated)
# MCP_DEFAULT_QUERY_LANGUAGES=en,no
# Minimum confidence threshold for language detection (0.0 - 1.0)
# MCP_LANGUAGE_CONFIDENCE_THRESHOLD=0.2

# Documentation Crawler Configuration
# MCP_CRAWL_TIMEOUT_MS=30000
# MCP_CRAWL_MAX_RESPONSE_BYTES=10485760
# MCP_CRAWL_REQUEST_DELAY_MS=100

# Similarity threshold for search results (0.0 to 1.0, where 1.0 = perfect match)
# Default: 0.5 (balanced precision/recall)
MCP_SIMILARITY_THRESHOLD=0.5

# Default maximum search results returned
MCP_MAX_SEARCH_RESULTS=10

# Number of parallel workers for chunking (adjust based on system resources)
# Recommended: 2-4 for 4-8GB RAM, 4-8 for 16+GB RAM
MCP_MAX_WORKERS=4

# Batch size for embedding generation (reduces API calls)
MCP_EMBEDDING_BATCH_SIZE=10

# Embedding dimension (default: 2048)
# Must match the dimension of your embedding model
# llama-nemotron-embed-1b-v2 produces 2048 dimensions
MCP_EMBEDDING_DIMENSION=2048
# AI provider configuration (OpenAI-compatible only)
# Supports LM Studio (local) and synthetic.new (remote)
MCP_AI_PROVIDER=openai
MCP_AI_BASE_URL=http://localhost:1234
MCP_AI_MODEL=ministral-3-8b-instruct-2512
MCP_AI_API_KEY=your-openai-compatible-api-key
MCP_AI_MAX_CONTEXT_CHUNKS=12

# Tag generation configuration
# Enable automatic tag generation using AI provider (default: false)
# When enabled, tags are generated asynchronously in the background when documents are added
MCP_TAG_GENERATION_ENABLED=false

# Use AI-generated tags in query search filters (default: false)
# When enabled, query filters will match documents with both manual tags and AI-generated tags
MCP_GENERATED_TAGS_IN_QUERY=false

# ============================================================================
# Multi-Provider Configuration (NEW)
# ============================================================================
# Configure multiple embedding providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_EMBEDDING_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_EMBEDDING_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "llama-nemotron-embed-1b-v2"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.openai.com/v1",
#     "model": "text-embedding-3-small",
#     "apiKey": "sk-..."
#   }
# ]'

# Configure multiple AI search providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_AI_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_AI_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "ministral-3-8b-instruct-2512"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.synthetic.new/openai/v1",
#     "model": "glm-4.7",
#     "apiKey": "sk-..."
#   }
# ]'

# Provider health check configuration
# Number of consecutive failures before marking a provider as unhealthy (default: 3)
MCP_PROVIDER_FAILURE_THRESHOLD=3

# Time in milliseconds before retrying an unhealthy provider (default: 300000 = 5 minutes)
MCP_PROVIDER_RECOVERY_TIMEOUT=300000

# ============================================================================
# Request Timeout Configuration
# ============================================================================
# All timeout values are in milliseconds (positive integers only)
# Timeout hierarchy: specific → global → default (30000ms)

# Global default timeout for all HTTP requests (default: 30000 = 30 seconds)
# This value is used when no specific timeout is configured
# MCP_REQUEST_TIMEOUT_MS=30000

# AI search specific timeout (optional)
# Overrides global timeout for AI search requests (search_documents_with_ai)
# Useful when AI providers are slow or have variable response times
# MCP_AI_SEARCH_TIMEOUT_MS=60000

# Embedding generation specific timeout (optional)
# Overrides global timeout for embedding API requests
# Increase this if using large models or slow embedding providers
# MCP_EMBEDDING_TIMEOUT_MS=45000

# Example configurations:
# - Fast local setup: MCP_REQUEST_TIMEOUT_MS=15000
# - Slow remote APIs: MCP_REQUEST_TIMEOUT_MS=60000
# - Different timeouts per operation:
#   MCP_REQUEST_TIMEOUT_MS=30000
#   MCP_AI_SEARCH_TIMEOUT_MS=120000
#   MCP_EMBEDDING_TIMEOUT_MS=45000
# Note: Values must be positive integers. Non-numeric, zero, or negative values
# will be rejected and the fallback will be used instead.

# ============================================================================
# Reranking Configuration
# ============================================================================
# Enable/disable reranking (default: true - opt-out)
# When enabled, queries use two-stage retrieval: vector search → reranking
# Set to 'false' to disable and use vector-only search
MCP_RERANKING_ENABLED=true

# Reranking provider: 'cohere', 'jina', 'openai', 'custom', or 'lmstudio' (default: 'cohere')
# IMPORTANT: The provider must match the base URL for reranking to work correctly
MCP_RERANKING_PROVIDER=cohere

# API base URL for reranking provider
# IMPORTANT: This URL must match your chosen provider!
# - Cohere: https://api.cohere.ai/v1 (requires API key)
# - Jina AI: https://api.jina.ai/v1 (requires API key)
# - OpenAI: https://api.openai.com/v1 (requires API key)
# - LM Studio (local): http://localhost:1234/v1 or http://127.0.0.1:1234/v1 (no API key needed)
# - Custom: Your custom OpenAI-compatible endpoint
MCP_RERANKING_BASE_URL=https://api.cohere.ai/v1

# API key for reranking provider
# REQUIRED for: Cohere, Jina AI, OpenAI
# OPTIONAL for: Custom endpoints (check your provider's requirements)
# NOT NEEDED for: LM Studio (local)
# 
# IMPORTANT: Replace 'your-reranking-api-key' with your actual API key
# For Cohere: Get your key at https://dashboard.cohere.ai/api-keys
# For Jina AI: Get your key at https://jina.ai/
# For OpenAI: Get your key at https://platform.openai.com/api-keys
MCP_RERANKING_API_KEY=your-reranking-api-key

# Reranking model to use
# IMPORTANT: The model must be supported by your chosen provider
# - Cohere: rerank-multilingual-v3.0 (recommended for multilingual support)
#           rerank-english-v3.0 (English-only, faster)
# - Jina AI: jina-reranker-v3-mlx
# - OpenAI: gpt-4o, gpt-4o-mini, or any model that supports reranking
# - LM Studio: Any instruction-following model (e.g., ministral-3-8b-instruct-2512)
#              Uses chat completions endpoint for reranking
# - Custom: Your custom model name
MCP_RERANKING_MODEL=rerank-multilingual-v3.0

# Maximum number of candidates to rerank (default: 50)
# Higher values = better quality but slower
# Recommended: 50 for top 10 results, 100 for top 20 results
MCP_RERANKING_CANDIDATES=50

# Number of results to return after reranking (default: 10)
# Must be less than or equal to MCP_RERANKING_CANDIDATES
MCP_RERANKING_TOP_K=10

# Request timeout for reranking API calls in milliseconds (default: 30000)
# Increase if using slow providers or large candidate counts
MCP_RERANKING_TIMEOUT=30000

# ============================================================================
# Example Configurations (choose one and uncomment)
# ============================================================================

# Example 1: Cohere (recommended for multilingual support)
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=cohere
# MCP_RERANKING_BASE_URL=https://api.cohere.ai/v1
# MCP_RERANKING_API_KEY=your-cohere-api-key-here
# MCP_RERANKING_MODEL=rerank-multilingual-v3.0

# Example 2: Jina AI (optimized for code and technical docs)
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=jina
# MCP_RERANKING_BASE_URL=https://api.jina.ai/v1
# MCP_RERANKING_API_KEY=your-jina-api-key-here
# MCP_RERANKING_MODEL=jina-reranker-v3-mlx

# Example 3: OpenAI
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=openai
# MCP_RERANKING_BASE_URL=https://api.openai.com/v1
# MCP_RERANKING_API_KEY=sk-your-openai-api-key-here
# MCP_RERANKING_MODEL=gpt-4o

# Example 4: LM Studio (local, uses chat completions for reranking)
# IMPORTANT: LM Studio doesn't support /v1/rerank endpoint
# Instead, it uses /v1/chat/completions with a custom prompt to rank documents
# 
# Prerequisites:
# - LM Studio server must be running on the specified port
# - Load an instruction-following model (e.g., ministral-3-8b-instruct-2512)
# - The model should be capable of understanding ranking instructions
#
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=lmstudio
# MCP_RERANKING_BASE_URL=http://localhost:1234/v1
# MCP_RERANKING_API_KEY=
# MCP_RERANKING_MODEL=ministral-3-8b-instruct-2512
#
# Note: LM Studio reranking uses chat completions, which may be slower than
# dedicated reranking APIs. Consider increasing MCP_RERANKING_TIMEOUT if needed.

# Example 5: Custom OpenAI-compatible endpoint
# MCP_RERANKING_ENABLED=true
# MCP_RERANKING_PROVIDER=custom
# MCP_RERANKING_BASE_URL=https://your-custom-endpoint.com/v1
# MCP_RERANKING_API_KEY=your-custom-api-key-if-needed
# MCP_RERANKING_MODEL=your-model-name

# ============================================================================
# MLX Reranker Configuration (Apple Silicon Only)
# ============================================================================
# Enable automatic MLX configuration on Apple Silicon (default: true)
# When enabled, the server will automatically detect Apple Silicon and configure MLX
# Set to 'false' to disable auto-configuration and use manual configuration
MCP_RERANKING_AUTO_CONFIGURE_MLX=true

# Custom path to MLX model directory (optional)
# If not set, defaults to $MCP_BASE_DIR/models/mlx-reranker
# Only set this if you want to use a custom location for MLX models
# MCP_RERANKING_MLX_MODEL_PATH=~/.saga/models/mlx-reranker

# Custom path to UV executable for MLX (optional)
# If not set, the server will search for 'uv' in the system PATH
# UV is required for MLX reranker to avoid polluting the OS Python environment
# Install UV from: https://docs.astral.sh/uv/getting-started/installation/
# MCP_RERANKING_MLX_UV_PATH=/usr/local/bin/uv

# Custom path to Python executable for MLX (deprecated, for backward compatibility)
# This option is deprecated. Use MCP_RERANKING_MLX_UV_PATH instead.
# If not set, UV will use its own Python environment
# MCP_RERANKING_MLX_PYTHON_PATH=/usr/local/bin/python3

# Example: Manual MLX configuration (auto-configure disabled)
# MCP_RERANKING_AUTO_CONFIGURE_MLX=false
# MCP_RERANKING_MLX_MODEL_PATH=/custom/path/to/mlx/models
# MCP_RERANKING_MLX_UV_PATH=/custom/path/to/uv

# ============================================================================
# Troubleshooting Tips
# ============================================================================
# If reranking is not working:
# 1. Check that MCP_RERANKING_ENABLED=true
# 2. Verify MCP_RERANKING_PROVIDER matches your base URL
# 3. Ensure MCP_RERANKING_API_KEY is set (unless using LM Studio or custom)
# 4. Confirm MCP_RERANKING_MODEL is supported by your provider
# 5. Check the server logs for detailed error messages
# 6. Test your API key and endpoint with curl or a similar tool
# 7. If using LM Studio:
#    - Ensure the server is running on the specified port
#    - Verify an instruction-following model is loaded
#    - Consider increasing MCP_RERANKING_TIMEOUT (chat completions may be slower)
#    - Check that the model can understand and follow ranking instructions
# 8. If using MLX on Apple Silicon:
#    - Ensure you're on an Apple Silicon Mac (M1/M2/M3/M4)
#    - Install UV package manager: https://docs.astral.sh/uv/getting-started/installation/
#      macOS/Linux: curl -LsSf https://astral.sh/uv/install.sh | sh
#      Windows: powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
#    - Verify UV installation: uv --version
#    - UV will automatically create an isolated environment with MLX dependencies
#    - Check that the model was downloaded successfully
#    - Review the MLX_README.md for detailed setup instructions

# ============================================================================
# Database v1.0 Optimization Configuration (NEW)
# ============================================================================

# HNSW Index Configuration
# Enable HNSW vector indexes for 2-5x faster queries (default: true)
MCP_USE_HNSW=true
# HNSW M parameter - max connections per node (default: 16)
MCP_HNSW_M=16
# HNSW efConstruction - build-time search depth (default: 200)
MCP_HNSW_EF_CONSTRUCTION=200
# HNSW ef - query-time search depth (default: 50)
MCP_HNSW_EF=50
# Vector count thresholds for parameter scaling
MCP_HNSW_MEDIUM_THRESHOLD=1000000
MCP_HNSW_LARGE_THRESHOLD=10000000

# Connection Pool Configuration
# Enable connection pooling (default: true)
MCP_CONNECTION_POOL_ENABLED=true
# Maximum pool size (default: 100)
MCP_CONNECTION_POOL_MAX=100
# Minimum pool size (default: 10)
MCP_CONNECTION_POOL_MIN=10
# Idle timeout in milliseconds (default: 30000)
MCP_CONNECTION_POOL_IDLE_TIMEOUT=30000
# Acquire timeout in milliseconds (default: 10000)
MCP_CONNECTION_POOL_ACQUIRE_TIMEOUT=10000
# Maximum connection lifetime in milliseconds (default: 3600000)
MCP_CONNECTION_POOL_MAX_LIFETIME=3600000

# Cache Configuration (L1 - In-Memory)
# Document cache size (default: 1000)
MCP_DOCUMENT_CACHE_SIZE=1000
# Chunk cache size (default: 5000)
MCP_CHUNK_CACHE_SIZE=5000
# Query result cache size (default: 100)
MCP_QUERY_CACHE_SIZE=100
# Cache TTL in milliseconds (default: 60000)
MCP_CACHE_TTL=60000

# Cache Configuration (L2 - Redis) - Optional
# Enable L2 Redis cache (default: false)
MCP_CACHE_L2_ENABLED=false
# Redis host (default: localhost)
MCP_REDIS_HOST=localhost
# Redis port (default: 6379)
MCP_REDIS_PORT=6379
# Redis database number (default: 0)
MCP_REDIS_DB=0
# Redis password (optional)
# MCP_REDIS_PASSWORD=your-redis-password
# L2 cache TTL in milliseconds (default: 300000)
MCP_CACHE_L2_TTL=300000
# Key prefix for cache entries (default: saga:)
MCP_CACHE_KEY_PREFIX=saga:

# Query Cache (L1) advanced knobs (applies to query-cache layer)
# MCP_CACHE_L1_MAX_SIZE=1000
# MCP_CACHE_L1_TTL=60000

# Query Optimizer Configuration
# Enable filter pushdown optimization (default: true)
MCP_QUERY_FILTER_PUSHDOWN=true
# Enable parallelization (default: true)
MCP_QUERY_PARALLELIZATION=true
# Parallelization threshold (default: 20)
MCP_QUERY_PARALLEL_THRESHOLD=20
# Cost reduction target (default: 0.5)
MCP_QUERY_COST_REDUCTION_TARGET=0.5

# Backup Configuration
# Backup schedule (cron expression, default: daily at 2 AM)
MCP_BACKUP_SCHEDULE=0 2 * * *
# Number of backups to retain (default: 7)
MCP_BACKUP_RETENTION=7
# Backup destination directory (default: <db_path>/backups)
# MCP_BACKUP_DESTINATION=~/.saga/backups
# Enable backup compression (default: true)
MCP_BACKUP_COMPRESSION=true
# Backup file prefix (default: saga-backup)
MCP_BACKUP_PREFIX=saga-backup

# Data Integrity Validation Configuration
# Validate UUIDs (default: true)
MCP_VALIDATE_UUIDS=true
# Validate content hashes (default: true)
MCP_VALIDATE_HASHES=true
# Validate relationships (default: true)
MCP_VALIDATE_RELATIONSHIPS=true
# Validate timestamps (default: true)
MCP_VALIDATE_TIMESTAMPS=true
# Check for orphaned records (default: true)
MCP_CHECK_ORPHANED=true
# Maximum validation issues to report (default: 1000)
MCP_MAX_VALIDATION_ISSUES=1000

# ============================================================================
# Testing / Diagnostics (optional)
# ============================================================================
# Use mock embeddings in tests or local dev
# MCP_USE_MOCK_EMBEDDINGS=true
# Force stub fetch in tests
# MCP_FORCE_STUB_FETCH=true
