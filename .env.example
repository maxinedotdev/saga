# Base directory for all data storage (documents, uploads, vector DB, etc.)
# Use ~/.doc-mcp/ for shared access between clients
# MCP_BASE_DIR=~/.doc-mcp/
MCP_BASE_DIR=~/.saga/

# LanceDB vector database configuration
# MCP_LANCE_DB_PATH is optional. If not set, defaults to $MCP_BASE_DIR/data/lancedb
# Only set this if you need to use a custom location
# MCP_LANCE_DB_PATH=~/.saga/data/lancedb

# Embedding provider configuration
# For OpenAI-compatible provider (e.g., LM Studio on http://127.0.0.1:1234):
# Default model is text-embedding-multilingual-e5-large-instruct (multilingual, high performance)
# Override with MCP_EMBEDDING_MODEL if you prefer a different model
MCP_EMBEDDING_PROVIDER=transformers
MCP_EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2
MCP_EMBEDDING_BASE_URL=http://127.0.0.1:1234
MCP_EMBEDDING_API_KEY=your-embedding-api-key
MCP_INDEXING_ENABLED=true
MCP_CACHE_SIZE=1000
MCP_PARALLEL_ENABLED=true
MCP_MAX_WORKERS=4
MCP_STREAMING_ENABLED=true
MCP_STREAM_CHUNK_SIZE=65536
MCP_STREAM_FILE_SIZE_LIMIT=10485760

# Chunking Configuration
# MCP_CHUNK_SIZE and MCP_CHUNK_OVERLAP can override intelligent defaults
# Set to -1 to use intelligent chunker's content-type-specific defaults
MCP_CHUNK_SIZE=-1
MCP_CHUNK_OVERLAP=-1

# Similarity threshold for search results (0.0 to 1.0, where 1.0 = perfect match)
# Default: 0.0 (allow all results)
MCP_SIMILARITY_THRESHOLD=0.0

# Default maximum search results returned
MCP_MAX_SEARCH_RESULTS=10

# Number of parallel workers for chunking (adjust based on system resources)
# Recommended: 2-4 for 4-8GB RAM, 4-8 for 16+GB RAM
MCP_MAX_WORKERS=4

# Batch size for embedding generation (reduces API calls)
MCP_EMBEDDING_BATCH_SIZE=10

# AI provider configuration (OpenAI-compatible only)
# Supports LM Studio (local) and synthetic.new (remote)
MCP_AI_BASE_URL=http://127.0.0.1:1234
MCP_AI_MODEL=ministral-3-8b-instruct-2512
MCP_AI_API_KEY=your-openai-compatible-api-key
MCP_AI_MAX_CONTEXT_CHUNKS=12

# Tag generation configuration
# Enable automatic tag generation using AI provider (default: false)
# When enabled, tags are generated asynchronously in the background when documents are added
MCP_TAG_GENERATION_ENABLED=false

# Use AI-generated tags in query search filters (default: false)
# When enabled, query filters will match documents with both manual tags and AI-generated tags
MCP_GENERATED_TAGS_IN_QUERY=false

# ============================================================================
# Multi-Provider Configuration (NEW)
# ============================================================================
# Configure multiple embedding providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_EMBEDDING_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_EMBEDDING_PROVIDERS='[
#   {
#     "provider": "transformers",
#     "priority": 1,
#     "modelName": "Xenova/all-MiniLM-L6-v2"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.openai.com/v1",
#     "model": "text-embedding-3-small",
#     "apiKey": "sk-..."
#   }
# ]'

# Configure multiple AI search providers with priority-based fallback
# If set, takes precedence over single-provider configuration (MCP_AI_*)
# Format: JSON array with priority (lower = higher priority)
# MCP_AI_PROVIDERS='[
#   {
#     "provider": "openai",
#     "priority": 1,
#     "baseUrl": "http://127.0.0.1:1234",
#     "model": "ministral-3-8b-instruct-2512"
#   },
#   {
#     "provider": "openai",
#     "priority": 2,
#     "baseUrl": "https://api.synthetic.new/openai/v1",
#     "model": "glm-4.7",
#     "apiKey": "sk-..."
#   }
# ]'

# Provider health check configuration
# Number of consecutive failures before marking a provider as unhealthy (default: 3)
MCP_PROVIDER_FAILURE_THRESHOLD=3

# Time in milliseconds before retrying an unhealthy provider (default: 300000 = 5 minutes)
MCP_PROVIDER_RECOVERY_TIMEOUT=300000
